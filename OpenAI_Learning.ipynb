{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V6Xhpsgvjm9_"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Understanding of OpenAI API"
      ],
      "metadata": {
        "id": "iWjgLn4iGKhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get API response**\n"
      ],
      "metadata": {
        "id": "CJNHbrQhqhKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key = \"")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[{\"role\":\"user\",\"content\":\"How many days are in October?\"}]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "DmY9Yg72krBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a30f60-6341-4ee5-8b25-f5798bf3263f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-B68H178WOpdYFMzuBgUC1rNxU6oyC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='October has 31 days.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1740801471, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sLdJyainIUE",
        "outputId": "4b91717f-73db-4c71-a885-78c3122d242c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "October has 31 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Controlling response randomness**\n",
        "*   `temperature`: control on determinism\n",
        "*   Range from `0` (highly deterministic) to `2` (very random)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3gMTOhrBp9jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[{\"role\":\"user\",\"content\":\"Life is like a box of chocolates.\"}],\n",
        "    temperature=2\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0-Us0Zrqa0g",
        "outputId": "be88cdf1-93cf-4bdc-8ec9-06dc151980f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-B68HCFgxdTCfrG146vNhtdfjKWTgL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"...you never know what you\\'re gonna get.\" This famous line, spoken by Forrest Gump, serves as a metaphor about the unpredictability of life. Just as a box of chocolates contains various flavors and experiences, life presents us with a variety of circumstances, each unique and unknown until we try everything out. It encourages embracing uncertainty and viewing experiences as opportunities—even if they aren\\'t what we expected. How do you think this captures your own experiences in life?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1740801482, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=91, prompt_tokens=15, total_tokens=106, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Content transformation**\n",
        "--- Changeing text based on an instruction\n",
        "*   Find and replace\n",
        "*   Summarization\n",
        "*   Copyediting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFCvvfWirLjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and Replace\n",
        "\n",
        "prompt = \"\"\"\n",
        "update name to Maarten, pronouns to he/him, and job title to Senior Content Developer in the following text:\n",
        "\n",
        "Joanne is a Content Developer at DataCamp. Here favorite programming language is R, which she uses for her statistical analyses.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgAUKoIP2efU",
        "outputId": "23af6c33-f341-43af-aea0-2aff1c6224f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maarten is a Senior Content Developer at DataCamp. His favorite programming language is R, which he uses for his statistical analyses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new content\n",
        "\n",
        "prompt = \"Create a tagline for a new hot dog stand.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3PYM9kp1cbL",
        "outputId": "bf83620c-33b3-4af5-fce3-ed306226203b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Relish the Flavor, Enjoy the Fun!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Controlling response length**\n",
        "\n",
        "\n",
        "*   ```max_tokens```: control on length\n",
        "*   Increasing ```max_tokens``` increases cost\n",
        "\n"
      ],
      "metadata": {
        "id": "G0Z5y85y4TWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"user\", \"content\": \"Write a haiku about AI.\"}],\n",
        "    max_tokens = 30\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9YgN7gA4ilN",
        "outputId": "a9c43101-edcd-422b-eb7c-804f3b0fe736"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines of code converge,  \n",
            "Whispers of thought, dreams alive—  \n",
            "Silent minds in bytes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis and classification"
      ],
      "metadata": {
        "id": "NkpZfO7f5x3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification tasks**\n",
        "\n",
        "*   Task that involves assgining a label to information\n",
        "  *   identifying the language from text\n",
        "  *   Categorization\n",
        "  * Classifying sentiment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6xUZWjR56SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizing Animals\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"user\", \"content\": \"Classify the following animals into categories: zebra, crocodile, blue whale, polar bear, salmon, dog.\"}],\n",
        "    max_tokens = 50\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKUsmwUf56Fl",
        "outputId": "5d0bf299-b850-4123-b89e-30e5561ecb83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the classifications of the given animals into categories based on their characteristics:\n",
            "\n",
            "**Mammals:**\n",
            "- Zebra\n",
            "- Blue Whale\n",
            "- Polar Bear\n",
            "- Dog\n",
            "\n",
            "**Fish:**\n",
            "- Salmon\n",
            "\n",
            "**Reptiles:**\n",
            "- Cro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifying sentiment\n",
        "\n",
        "prompt = \"\"\"Classify sentiment in the following statements:\n",
        "1. The service was very slow\n",
        "2. The steak was awfully tasty!\n",
        "3. Meal was decent, but I've had better.\n",
        "4. My food was delayed, but drinks were good.\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"user\", \"content\": prompt}],\n",
        "    max_tokens = 80\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl_nLrey54cz",
        "outputId": "9ecf70c5-be51-4027-bb4c-8b8cb241ff8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here’s the sentiment classification for each statement:\n",
            "\n",
            "1. The service was very slow - Negative\n",
            "2. The steak was awfully tasty! - Positive\n",
            "3. Meal was decent, but I've had better. - Neutral\n",
            "4. My food was delayed, but drinks were good. - Mixed (Negative for food delay, Positive for drinks)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot vs. one-shot vs. few-shot prompting**\n",
        "\n",
        "*  **Zero-shot** prompting: no examples provided\n",
        "\n",
        "In-context learning:\n",
        "*   **One-shot** prompting: one examples provided\n",
        "*   **Few-shot** prompting: multiples examples provided\n"
      ],
      "metadata": {
        "id": "tyazKSLk52Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot prompting\n",
        "\n",
        "# Classifying sentiment\n",
        "\n",
        "prompt = \"\"\"Classify sentiment in the following statements:\n",
        "The service was very slow //Disgruntled\n",
        "Meal was decent, but I've had better. //\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJQ7G2B940Tm",
        "outputId": "1f654d2b-84b9-4f87-ac1b-1f08c5ea06bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The service was very slow - Disgruntled\n",
            "2. Meal was decent, but I've had better - Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat completions with GPT"
      ],
      "metadata": {
        "id": "dEllPNvl8nkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Single-turn* task:\n",
        "* Text generation\n",
        "* Text transformation\n",
        "* Classification\n",
        "\n",
        "*Multi-turn* conversations\n",
        "* -> build on previous prompts and response"
      ],
      "metadata": {
        "id": "w7LOQieX84et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roles**\n",
        "* **System:** contraols assistant's behavior\n",
        "* **User:** instruct trhe assistant\n",
        "* **Assistant:** response to user instruction\n",
        "  * Can aslo be written by the user to provide examples"
      ],
      "metadata": {
        "id": "bfp8iaMz9k0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"system\", \"content\": \"You are a data science tutor who speaks concisely.\"}, # set role as system\n",
        "                {\"role\":\"user\", \"content\": \"What is the difference between mutable and imuutable objects?\"}] # set role as user\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcZR895u8cre",
        "outputId": "8dd82455-7342-496a-f83e-f5011965b16e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutable objects can be changed after they are created, meaning their state or contents can be modified. Examples include lists and dictionaries in Python. \n",
            "\n",
            "Immutable objects, on the other hand, cannot be altered once created. Any modification results in the creation of a new object. Examples include tuples and strings in Python. \n",
            "\n",
            "In summary: \n",
            "\n",
            "- **Mutable**: Changeable (e.g., lists, dictionaries)\n",
            "- **Immutable**: Unchangeable (e.g., tuples, strings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-turn chat completions with GPT**"
      ],
      "metadata": {
        "id": "5LUPKMr4-p5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages = [{\"role\":\"system\", \"content\": \"You are a data science tutor who speaks concisely.\"}, # set role as system\n",
        "                {\"role\":\"user\", \"content\": \"How do you define a Python list?\"}, # set role as user\n",
        "                {\"role\":\"assistant\", \"content\": \"Lists are defined by enclosing a comma-separated sequence of objects inside square brackets [].\"}, # set role as assistant\n",
        "                {\"role\":\"user\", \"content\": \"What is the difference between mutable and imuutable objects?\"}] # set role as user\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_t-Dy70-h6g",
        "outputId": "1473dec1-6753-48cd-c1c2-9932d617b5d3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutable objects can be changed after creation (e.g., lists, dictionaries), while immutable objects cannot be modified (e.g., tuples, strings).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Storing Responses**\n",
        "* Create conversation history\n",
        "* Create back-and-forth conversations"
      ],
      "metadata": {
        "id": "l4IBfzMN_p3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\":\"system\", \"content\": \"You are a data science tutor who speaks concisely.\"}]\n",
        "\n",
        "user_qs = [\"why is Python so popular?\", \"Summarize this in one sentence.\"]\n",
        "\n",
        "for q in user_qs:\n",
        "  print(\"User: \", q)\n",
        "  user_dict = {\"role\":\"user\", \"content\": q}\n",
        "  messages.append(user_dict)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model = \"gpt-4o-mini\",\n",
        "      messages = messages\n",
        "  )\n",
        "\n",
        "  assistant_dict = {\"role\":\"assistant\", \"content\": response.choices[0].message.content}\n",
        "  messages.append(assistant_dict)\n",
        "  print(\"Assistant: \", response.choices[0].message.content, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwhS93-g_jJ9",
        "outputId": "9fc1a3a7-8feb-4f0b-a27d-1f1b6324374c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User:  why is Python so popular?\n",
            "Assistant:  Python is popular due to its simplicity and readability, versatile libraries (like Pandas, NumPy, and TensorFlow), strong community support, and applications across various fields such as web development, data analysis, artificial intelligence, and more. Its ease of learning makes it accessible for beginners and professionals alike. \n",
            "\n",
            "User:  Summarize this in one sentence.\n",
            "Assistant:  Python's popularity stems from its simplicity, versatility, extensive libraries, strong community support, and wide-ranging applications across various fields. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Moderation Model"
      ],
      "metadata": {
        "id": "6l-FF44dF3e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text moderation**\n",
        "* Identifying inappropriate content - business usage: harmful and offensive tweets post\n",
        "\n",
        "Traditionally,\n",
        "* Moderators flag content by-han\n",
        "  * Time-consuming\n",
        "* **Keyword pattern matching**\n",
        "  * Lack nuance and understanding of context\n",
        "* **OpenAI Moderation**\n",
        "  * Identify violations of terms or use\n",
        "  * Differentiate volation type by category\n",
        "    * Violence\n",
        "    * Hate Speech"
      ],
      "metadata": {
        "id": "xWIEYuE1BU32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating s moderations request\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key = \"")\n",
        "\n",
        "response = client.moderations.create(\n",
        "    model = \"text-moderation-latest\",\n",
        "    input = \"I could kill for a hamburger.\"\n",
        ")"
      ],
      "metadata": {
        "id": "FyqNGZefAGJs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.results[0].category_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy7iiF9iDSUk",
        "outputId": "eea25d92-64eb-4e1c-d219-39cd08d4123d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CategoryScores(harassment=0.0002501912822481245, harassment_threatening=0.0002896765654440969, hate=2.9961185646243393e-05, hate_threatening=1.946206111824722e-06, illicit=None, illicit_violent=None, self_harm=0.0002134342648787424, self_harm_instructions=7.010227420778392e-08, self_harm_intent=5.6784261687425897e-05, sexual=2.7037902327720076e-05, sexual_minors=5.88963985137525e-07, violence=0.03668493404984474, violence_graphic=0.00024499755818396807, self-harm=0.0002134342648787424, sexual/minors=5.88963985137525e-07, hate/threatening=1.946206111824722e-06, violence/graphic=0.00024499755818396807, self-harm/intent=5.6784261687425897e-05, self-harm/instructions=7.010227420778392e-08, harassment/threatening=0.0002896765654440969)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the results\n",
        "\n",
        "* ```categories```\n",
        "  * ```true```/```false``` indicator of category violation\n",
        "* ```category_scores```\n",
        "  * Confidence of a violation\n",
        "  * Large numbers -> greater certainty of violation\n",
        "  * Number != probabilities\n",
        "* ```flagged```\n",
        "  * ```true```/```false``` indicator of violation\n",
        "\n",
        "Considerations for implementing moderation\n",
        "* Determine appropriate thresholds for each use case\n",
        "* Stricter thresholds may result in fewer *false negatives* (eg. school communications)\n",
        "* More Ienient thresholds may result in fewer *false positives* (eg. crime report)"
      ],
      "metadata": {
        "id": "8M8qA_ufCzjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Audio Model"
      ],
      "metadata": {
        "id": "FoYVEXRvGAc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Speech-to-Text Transcription with Whisper**\n",
        "Speech-to-text capacilities:\n",
        "* Transcribe audio\n",
        "* Translate and transcribe audio into English\n",
        "* Supports ```mp3```,```mp4```,```mpeg```,```mpga```,```m4a```,```wav```, and ```webm``` (25 MB limit)\n",
        "* Meeting transcripts\n",
        "* Video captions"
      ],
      "metadata": {
        "id": "NGtYZjkSEWN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading audio files\n",
        "# Example: transcribe meeting_recordings.mp3\n",
        "\n",
        "audio_file = open(\"meeting_recordings.mp3\", \"rb\") #rb: read binary\n",
        "\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    model = \"whisper-1\",\n",
        "    file = audio_file,\n",
        "    response_format = \"text\"\n",
        ")\n",
        "\n",
        "print(transcript)"
      ],
      "metadata": {
        "id": "Hc47BDq7DLGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Speech Translation with Whisper**"
      ],
      "metadata": {
        "id": "OgdOq3jGFqhL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4B5JLJYFwqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
